#!/usr/bin/env python2.7

import os
from decimal import Decimal

# Setup
ARTICLES = 'articles'
CORPORATE = 'corporate'
ENRON_T_S = 'enron_t_s'
ENRON_TRAVEL_CLUB = 'enron_travel_club'
HEA_NESA = 'hea_nesa'
PERSONAL = 'personal'
SYSTEMS = 'systems'
TW_COMMERCIAL_GROUP = 'tw_commercial_group'
ALL_LABELS = [ARTICLES, CORPORATE, ENRON_T_S, ENRON_TRAVEL_CLUB, HEA_NESA, PERSONAL, SYSTEMS,
              TW_COMMERCIAL_GROUP]

label_to_numerical_value = {
    ARTICLES: '1.0',
    CORPORATE: '2.0',
    ENRON_T_S: '3.0',
    ENRON_TRAVEL_CLUB: '4.0',
    HEA_NESA: '5.0',
    PERSONAL: '6.0',
    SYSTEMS: '7.0',
    TW_COMMERCIAL_GROUP: '8.0',
}


def get_file_path_in_processed(file_name):
    return '../materials/data/processed/{}'.format(file_name)


def get_lines_from_file(file_name):
    relative_file_location = get_file_path_in_processed(file_name)
    absolute_file_location = os.path.join(os.path.dirname(__file__), relative_file_location)
    lines = tuple(open(absolute_file_location, 'r'))
    return lines


def get_lines_for_label(label_id):
    file_name = '{}.train.txt'.format(label_id)
    return get_lines_from_file(file_name)


label_to_lines = {label: get_lines_for_label(label) for label in ALL_LABELS}

vocabulary_lines = get_lines_from_file('vocabulary.txt')


def line_to_word_and_count(line):
    word, string_count = line.split()
    count = int(string_count)
    return word, count

all_vocab_words = [line_to_word_and_count(line)[0] for line in vocabulary_lines]

label_to_all_words = {label: [line_to_word_and_count(line)[0] for line in label_to_lines[label]]
                      for label in ALL_LABELS}


def get_label_and_word_to_count():
    label_to_word_count_map = dict()
    for label in ALL_LABELS:
        # start with empty dict. Will iterate over lines for label and add to dict
        label_to_word_count_map[label] = dict()
        for line in label_to_lines[label]:
            word, count = line_to_word_and_count(line)
            label_to_word_count_map[label][word] = count
    return label_to_word_count_map


# basically a function with state
def get_vocab_to_word_count():
    word_to_count = dict()
    for line in vocabulary_lines:
        word, count = line_to_word_and_count(line)
        word_to_count[word] = count
    return word_to_count


label_and_word_to_count = get_label_and_word_to_count()
vocab_to_count = get_vocab_to_word_count()

size_of_vocab = 0
for vocab_word in all_vocab_words:
    size_of_vocab += vocab_to_count[vocab_word]

size_of_label = {label: 0 for label in ALL_LABELS}
for label in ALL_LABELS:
    for vocab_word in label_to_all_words[label]:
        size_of_label[label] += label_and_word_to_count[label][vocab_word]

# ===================
# lets do algorithm 1


def build_naive_bayes():
    probability_of_label = {}
    probability_of_word_given_label = {label: dict() for label in ALL_LABELS}

    # V <- set of all tokens occurring in S
    # this is all_vocab_words

    # for y in Y do
    for label in ALL_LABELS:
        # p(y) <- |S_y| / |S|
        prob_of_cur_label = Decimal(size_of_label[label]) / Decimal(size_of_vocab)
        probability_of_label[label] = prob_of_cur_label

        # C_y <- corpus generated by concatenating S_y {these are [directory].train.txt}
        # already set up

        # for w in V do
        for word in all_vocab_words:

            # n_w <- number of times w occurs in C_y
            num_occurances_of_word_for_label = label_and_word_to_count[label].get(word, Decimal(0))

            # p(x_i = w | y) <- (n_w + 1)/(|C_y| + |V|)
            probability_of_word_given_label[label][word] = \
                Decimal(num_occurances_of_word_for_label + 1) / \
                Decimal(size_of_label[label] + size_of_vocab)
    return probability_of_label, probability_of_word_given_label


def save_predictions(lines):
    relative_file_location = '../materials/output/predictions.nb'
    absolute_file_location = os.path.join(os.path.dirname(__file__), relative_file_location)
    prediction_file = open(absolute_file_location, 'w')
    for line in lines:
        prediction_file.write('{}\n'.format(line))


def make_predictions(file_name):
    file_lines = get_lines_from_file(file_name)
    probability_of_label, probability_of_word_given_label = build_naive_bayes()

    predictions = []
    for line in file_lines:
        best_label = None
        best_probability = Decimal(0)
        words = line.split()
        for label in ALL_LABELS:
            cumulative_prob = probability_of_label[label]
            for word in words:
                cumulative_prob *= probability_of_word_given_label[label].get(word, Decimal(1))
            if cumulative_prob > best_probability:
                assert cumulative_prob != Decimal(1)  # something went wrong
                best_probability = cumulative_prob
                best_label = label
        assert best_label is not None
        best_label_numerical_value = label_to_numerical_value[best_label]
        predictions.append(best_label_numerical_value)
    save_predictions(predictions)

make_predictions('test.txt')
